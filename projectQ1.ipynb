{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. We treat motor UPDRS score as the response and use a variety of regression models to predict\n",
    "the motor UPDRS score of patients using six biomedical voice measures. </h2>\n",
    "\n",
    "<table><tr><th>Variable</th> <th>Description</th></tr> \n",
    "<tr><td>motor_updrs</td> <td> modified clinician’s motor UPDRS score</td></tr>\n",
    "<tr><td>Abs,PPQ5</td> <td> Measures of variation in fundamental frequency</td></tr>\n",
    "<tr><td>dB,APQ11</td> <td> Measures of variation in amplitude</td></tr>\n",
    "<tr><td>NHR</td> <td>A measure of ratio of noise to tonal components in the voice</td></tr>\n",
    "<tr><td>RPDE</td> <td>A nonlinear dynamical complexity measure</td></tr>\n",
    "</table>\n",
    "\n",
    "Table 1: Description of the response and predictors in park.csv file. The response is in the first\n",
    "row and the remaining rows describe the biomedical voice measures.\n",
    "\n",
    "Answer the following questions based on different types of regression models introduced in the\n",
    "class. Evaluate their predictive performance using mean square prediction error (MSPE) estimates\n",
    "based on 10-fold cross-validation.\n",
    "\n",
    "<h3>1. (10 pts) How does a linear regression model perform when predicting motor UPDRS scores using\n",
    "the given biomedical voice measures? Evaluate the model’s predictive performance</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>motor_updrs</th>\n",
       "      <th>Abs</th>\n",
       "      <th>PPQ5</th>\n",
       "      <th>dB</th>\n",
       "      <th>APQ11</th>\n",
       "      <th>NHR</th>\n",
       "      <th>RPDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.234932</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.00886</td>\n",
       "      <td>0.011626</td>\n",
       "      <td>0.50668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.014135</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.01323</td>\n",
       "      <td>0.061012</td>\n",
       "      <td>0.50328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.808876</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.02524</td>\n",
       "      <td>0.017954</td>\n",
       "      <td>0.62443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.147702</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.00891</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.05491</td>\n",
       "      <td>0.089232</td>\n",
       "      <td>0.57429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.270910</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.00581</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.02216</td>\n",
       "      <td>0.031666</td>\n",
       "      <td>0.55557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   motor_updrs       Abs     PPQ5     dB    APQ11       NHR     RPDE\n",
       "0     4.234932  0.000017  0.00137  0.136  0.00886  0.011626  0.50668\n",
       "1     2.014135  0.000033  0.00339  0.225  0.01323  0.061012  0.50328\n",
       "2     3.808876  0.000044  0.00272  0.287  0.02524  0.017954  0.62443\n",
       "3     4.147702  0.000063  0.00891  0.758  0.05491  0.089232  0.57429\n",
       "4     2.270910  0.000105  0.00581  0.404  0.02216  0.031666  0.55557"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('park.csv')\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and target variable\n",
    "X = data.iloc[:, 1:].values  # Features\n",
    "# X = data.iloc[1:, 1:].values  # Features\n",
    "\n",
    "y = data.iloc[:,0].values   # Target variable\n",
    "#y = data.iloc[0, 1:].values   # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Convert data to float\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "y_test = y_test.astype(float)\n",
    "\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "# Function to calculate list of mean squared prediction errors done by k-fold cross validation\n",
    "# Inputs: k-fold division, the model, X matrix and y matrix \n",
    "# Output: List of 9-folds mspe values\n",
    "# Process: Divide the data into k folds. For each fold, train the data on other k-1 folds\n",
    "# and calculate mspe on the current fold. Store the values in an array\n",
    "def kFoldMspeCalc(kFold, model, X, y):\n",
    "    # Define a matrix for storing mean square pred errors\n",
    "    mspeErrors = []\n",
    "    \n",
    "    # Perform 10-fold cross-validation\n",
    "    for train_index, test_index in kFold.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict the target variable\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Cross Validation Errors\n",
    "        mspeErrors.append(mean_squared_error(y_test,y_pred))\n",
    "    \n",
    "    return mspeErrors\n",
    "\n",
    "def testingMSPE(model, X_testing, y_testing):\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "        \n",
    "    # Predict the target variable\n",
    "    y_pred = model.predict(X_testing)\n",
    "\n",
    "    return(mean_squared_error(y_testing,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Model MSPE: 0.9642768220816269\n",
      "Fitting on Testing Data:  1.1802119869996937\n"
     ]
    }
   ],
   "source": [
    "# Initialize Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Define 10-fold cross-validation with shuffling and seed for reproducability\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Get list of mspes\n",
    "crossValErrors = kFoldMspeCalc(kf, model, X, y)\n",
    "\n",
    "# Print mean MSPE\n",
    "print(\"Linear Regression Model MSPE:\", np.mean(crossValErrors))\n",
    "\n",
    "print(\"Fitting on Testing Data: \", testingMSPE(model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An average MSPE of 1.037 in 10 fold cross training data is achieved. On testing data, the MSPE is 1.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. (20 pts) Extend the previous model to a polynomial regression of degree 2. Extend it further us-\n",
    "ing the lasso and ridge penalties and principal components regression. Compare their predictive\n",
    "performance with the previous model.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression Model MSPE: 1.4453703001709761\n",
      "Fitting on Testing Data:  1.1802119869996937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Initialize Linear Regression model\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "crossValErrors = kFoldMspeCalc(kf, model, X_poly, y)\n",
    "\n",
    "# Print mean MSPE\n",
    "print(\"Polynomial Regression Model MSPE:\", np.mean(crossValErrors))\n",
    "\n",
    "print(\"Fitting on Testing Data: \", testingMSPE(model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression Model MSPE at alpha = 0.001: 0.9592875498055763\n",
      "Fitting on Testing Data:  1.160594171255275\n",
      "Lasso Regression Model MSPE at alpha = 0.002: 0.958217827402993\n",
      "Fitting on Testing Data:  1.1624198944398119\n",
      "Lasso Regression Model MSPE at alpha = 0.003: 0.95860954145864\n",
      "Fitting on Testing Data:  1.1644268937888511\n",
      "Lasso Regression Model MSPE at alpha = 0.004: 0.9593607059585854\n",
      "Fitting on Testing Data:  1.1666160260195362\n",
      "Lasso Regression Model MSPE at alpha = 0.005: 0.9603093728997983\n",
      "Fitting on Testing Data:  1.1689872911318682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge;\n",
    "\n",
    "\n",
    "for alpha in [0.001, 0.002, 0.003, 0.004, 0.005 ]:\n",
    "    model = Lasso(alpha=alpha)\n",
    "    crossValErrors = kFoldMspeCalc(kf, model, X, y)\n",
    "    print(\"Lasso Regression Model MSPE at alpha = {:}:\".format(alpha), np.mean(crossValErrors))\n",
    "\n",
    "    print(\"Fitting on Testing Data: \", testingMSPE(model, X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression Model MSPE at alpha = 0.000050: 0.9750276771726953\n",
      "Fitting on Testing Data:  1.1626405044632477\n",
      "Ridge Regression Model MSPE at alpha = 0.000075: 0.9750590333182718\n",
      "Fitting on Testing Data:  1.162583761266616\n",
      "Ridge Regression Model MSPE at alpha = 0.000100: 0.9750756982454458\n",
      "Fitting on Testing Data:  1.1625401768388757\n",
      "Ridge Regression Model MSPE at alpha = 0.001000: 0.9750409471383943\n",
      "Fitting on Testing Data:  1.1616643023940239\n",
      "Ridge Regression Model MSPE at alpha = 0.001500: 0.9749398913476307\n",
      "Fitting on Testing Data:  1.161351479339823\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.00005, 0.000075, 0.0001, 0.001, 0.0015]:\n",
    "    model = Ridge(alpha=alpha)\n",
    "\n",
    "    crossValErrors = kFoldMspeCalc(kf, model, X, y)\n",
    "\n",
    "    print(\"Ridge Regression Model MSPE at alpha = {:5f}:\".format(alpha), np.mean(crossValErrors))\n",
    "    print(\"Fitting on Testing Data: \", testingMSPE(model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Component Regression Model MSPE with 6 components: 0.9642768220816261\n",
      "Fitting on Testing Data:  1.161351479339823\n",
      "Principal Component Regression Model MSPE with 5 components: 0.9751073434933973\n",
      "Fitting on Testing Data:  1.161351479339823\n",
      "Principal Component Regression Model MSPE with 4 components: 0.9781503117944551\n",
      "Fitting on Testing Data:  1.161351479339823\n",
      "Principal Component Regression Model MSPE with 3 components: 0.9649861842418479\n",
      "Fitting on Testing Data:  1.161351479339823\n",
      "Principal Component Regression Model MSPE with 2 components: 0.9583892408779737\n",
      "Fitting on Testing Data:  1.161351479339823\n",
      "Principal Component Regression Model MSPE with 1 components: 1.7402786517183522\n",
      "Fitting on Testing Data:  1.161351479339823\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "\n",
    "reg = LinearRegression() \n",
    "\n",
    "\n",
    "for n_components in [6,5,4,3,2,1]:\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    pipeline = Pipeline(steps=[('pca', pca), \n",
    "                           ('reg', reg)]) \n",
    "    crossValErrors = kFoldMspeCalc(kf, pipeline, X, y)\n",
    "\n",
    "    print(\"Principal Component Regression Model MSPE with \"+str(n_components)+\" components:\", np.mean(crossValErrors))\n",
    "    print(\"Fitting on Testing Data: \", testingMSPE(model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial features have the worst training error @ 1.44 and ridge, lasso and PCA haveing rate of 0.95. \n",
    "\n",
    "However all models have a comparable training error of around 1.16. Lasso regression seem to perform best here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. (10 pts) How does using the polynomial kernels of degrees 1, 2, 10, and 20 in kernel regression\n",
    "affect the prediction accuracy of motor UPDRS scores? Does the polynomial kernel beat the\n",
    "performance of the previous models for some degree? Justify the similarity between polynomial\n",
    "regressions used in the previous questions and kernel regressions used in this question.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Kernel MSPE with lambda: 0 and degree: 1 is: 392.5130814353853\n",
      "Polynomial Kernel MSPE with lambda: 0 and degree: 2 is: 3127.9680979666437\n",
      "Polynomial Kernel MSPE with lambda: 0 and degree: 10 is: 563.8374225738235\n",
      "Polynomial Kernel MSPE with lambda: 0 and degree: 20 is: 298.876355522254\n",
      "\n",
      "Polynomial Kernel MSPE with lambda: 0.1 and degree: 1 is: 0.9478480473229165\n",
      "Polynomial Kernel MSPE with lambda: 0.1 and degree: 2 is: 0.945033533703986\n",
      "Polynomial Kernel MSPE with lambda: 0.1 and degree: 10 is: 1.005677640441731\n",
      "Polynomial Kernel MSPE with lambda: 0.1 and degree: 20 is: 1.0678771844511354\n",
      "\n",
      "Polynomial Kernel MSPE with lambda: 0.5 and degree: 1 is: 0.9541942347464376\n",
      "Polynomial Kernel MSPE with lambda: 0.5 and degree: 2 is: 0.9467320045850471\n",
      "Polynomial Kernel MSPE with lambda: 0.5 and degree: 10 is: 0.9897347163833563\n",
      "Polynomial Kernel MSPE with lambda: 0.5 and degree: 20 is: 1.0181877000587871\n",
      "\n",
      "Polynomial Kernel MSPE with lambda: 1 and degree: 1 is: 0.9546783560102865\n",
      "Polynomial Kernel MSPE with lambda: 1 and degree: 2 is: 0.9496536649758491\n",
      "Polynomial Kernel MSPE with lambda: 1 and degree: 10 is: 0.9758525532702886\n",
      "Polynomial Kernel MSPE with lambda: 1 and degree: 20 is: 1.010071513743738\n",
      "\n",
      "lambda =1 yields the best results. Running predictions with lambda =1\n",
      "Fitting on Testing Data and degreee: 1:  1.130624285826553\n",
      "Fitting on Testing Data and degreee: 2:  1.1236737633303229\n",
      "Fitting on Testing Data and degreee: 10:  1.3730220696344009\n",
      "Fitting on Testing Data and degreee: 20:  1.4205508511913043\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# Ridge penalty values\n",
    "alphas = [0, 0.1, 0.5, 1]\n",
    "\n",
    "# Polynomial degree values\n",
    "gammas = [1, 2, 10, 20]\n",
    "mspe_array = np.zeros((len(alphas), len(gammas)))\n",
    "\n",
    "# Iterate over alphas and gammas, and fill the MSPE values\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "\n",
    "        model = KernelRidge(alpha=alpha, kernel=\"polynomial\", gamma = gamma, coef0=1)\n",
    "\n",
    "        crossValErrors = kFoldMspeCalc(kf, model, X, y)\n",
    "        # Print mean MSPE\n",
    "        print(\"Polynomial Kernel MSPE with lambda: \"+str(alpha)+\" and degree: \"+str(gamma)+\" is:\" ,np.mean(crossValErrors))\n",
    "    print()\n",
    "\n",
    "print(\"lambda =1 yields the best results. Running predictions with lambda =1\")\n",
    "for gamma in gammas:\n",
    "    model = KernelRidge(alpha=1, kernel=\"polynomial\", gamma = gamma, coef0=1)\n",
    "    print(\"Fitting on Testing Data and degreee: \"+str(gamma)+\": \", testingMSPE(model, X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial features with degree 2 seem to give the best testing performance and it seems that the data set is prone to overfitting\n",
    "\n",
    "\n",
    "Both methods aim to capture non-linear relationships in the data by introducing higher-order terms or mapping the data to a higher-dimensional feature space. \n",
    "\n",
    "In polynomial regression, we explicitly create the higher-order terms, which can become computationally expensive and lead to the curse of dimensionality as the degree of the polynomial increases. \n",
    "\n",
    "Kernel ridge regression with a polynomial kernel uses the kernel trick to implicitly map the data to a higher-dimensional space, which can be more computationally efficient and less prone to the curse of dimensionality.\n",
    "\n",
    "With certain regularization, the kernel performs slightly better, although higher degrees do not seem to lead to higher performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. (10 pts) How does the performance change if we use the radial basis function and Laplace (or exponential) kernels in the previous question?</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Kernel MSPE with lambda: 0 and degree: 1 is: 279191.5477681823\n",
      "Polynomial Kernel MSPE with lambda: 0 and degree: 2 is: 6071685.852212243\n",
      "Polynomial Kernel MSPE with lambda: 0 and degree: 10 is: 68787579.24764112\n",
      "Polynomial Kernel MSPE with lambda: 0 and degree: 20 is: 802963844.3469262\n",
      "\n",
      "Polynomial Kernel MSPE with lambda: 0.1 and degree: 1 is: 0.9563927390319306\n",
      "Polynomial Kernel MSPE with lambda: 0.1 and degree: 2 is: 0.9516024295480948\n",
      "Polynomial Kernel MSPE with lambda: 0.1 and degree: 10 is: 0.9941336414469303\n",
      "Polynomial Kernel MSPE with lambda: 0.1 and degree: 20 is: 1.0514991774381808\n",
      "\n",
      "Polynomial Kernel MSPE with lambda: 0.5 and degree: 1 is: 0.9747783163358681\n",
      "Polynomial Kernel MSPE with lambda: 0.5 and degree: 2 is: 0.9693558186727567\n",
      "Polynomial Kernel MSPE with lambda: 0.5 and degree: 10 is: 1.0041811775267389\n",
      "Polynomial Kernel MSPE with lambda: 0.5 and degree: 20 is: 1.076659020290951\n",
      "\n",
      "Polynomial Kernel MSPE with lambda: 1 and degree: 1 is: 0.9914488735424213\n",
      "Polynomial Kernel MSPE with lambda: 1 and degree: 2 is: 0.9872266810433754\n",
      "Polynomial Kernel MSPE with lambda: 1 and degree: 10 is: 1.0318343156167402\n",
      "Polynomial Kernel MSPE with lambda: 1 and degree: 20 is: 1.1227429212494295\n",
      "\n",
      "lambda =1 yields the best results. Running predictions with lambda =0.1\n",
      "Fitting on Testing Data and degreee: 1:  1.1358345293636702\n",
      "Fitting on Testing Data and degreee: 2:  1.135901020978372\n",
      "Fitting on Testing Data and degreee: 10:  1.1831601239082732\n",
      "Fitting on Testing Data and degreee: 20:  1.2384606343085012\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# Ridge penalty values\n",
    "alphas = [0, 0.1, 0.5, 1]\n",
    "\n",
    "# Polynomial degree values\n",
    "gammas = [1, 2, 10, 20]\n",
    "mspe_array = np.zeros((len(alphas), len(gammas)))\n",
    "\n",
    "# Iterate over alphas and gammas, and fill the MSPE values\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "\n",
    "        model = KernelRidge(alpha=alpha, kernel=\"rbf\", gamma = gamma)\n",
    "\n",
    "        crossValErrors = kFoldMspeCalc(kf, model, X, y)\n",
    "        # Print mean MSPE\n",
    "        print(\"Polynomial Kernel MSPE with lambda: \"+str(alpha)+\" and degree: \"+str(gamma)+\" is:\" ,np.mean(crossValErrors))\n",
    "    print()\n",
    "\n",
    "print(\"lambda =1 yields the best results. Running predictions with lambda =0.1\")\n",
    "for gamma in gammas:\n",
    "    model = KernelRidge(alpha=0.1, kernel=\"rbf\", gamma = gamma, coef0=1)\n",
    "    print(\"Fitting on Testing Data and degreee: \"+str(gamma)+\": \", testingMSPE(model, X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Kernel MSPE with lambda: 0 and degree: 1 is: 1.9243062863214973\n",
      "Polynomial Kernel MSPE with lambda: 0 and degree: 2 is: 1.801786290010964\n",
      "Polynomial Kernel MSPE with lambda: 0 and degree: 10 is: 1.9005569190553142\n",
      "Polynomial Kernel MSPE with lambda: 0 and degree: 20 is: 2.412808307906997\n",
      "\n",
      "Polynomial Kernel MSPE with lambda: 0.1 and degree: 1 is: 1.042216373223147\n",
      "Polynomial Kernel MSPE with lambda: 0.1 and degree: 2 is: 1.1137096215980862\n",
      "Polynomial Kernel MSPE with lambda: 0.1 and degree: 10 is: 1.7285373407054454\n",
      "Polynomial Kernel MSPE with lambda: 0.1 and degree: 20 is: 2.375799882746372\n",
      "\n",
      "Polynomial Kernel MSPE with lambda: 0.5 and degree: 1 is: 0.9960367493127593\n",
      "Polynomial Kernel MSPE with lambda: 0.5 and degree: 2 is: 1.0427232175527847\n",
      "Polynomial Kernel MSPE with lambda: 0.5 and degree: 10 is: 1.6942291353516439\n",
      "Polynomial Kernel MSPE with lambda: 0.5 and degree: 20 is: 2.425690652141559\n",
      "\n",
      "Polynomial Kernel MSPE with lambda: 1 and degree: 1 is: 1.0026442320335334\n",
      "Polynomial Kernel MSPE with lambda: 1 and degree: 2 is: 1.0502373056822687\n",
      "Polynomial Kernel MSPE with lambda: 1 and degree: 10 is: 1.7595897910217655\n",
      "Polynomial Kernel MSPE with lambda: 1 and degree: 20 is: 2.546079580276161\n",
      "\n",
      "lambda =1 yields the best results. Running predictions with lambda =0.5\n",
      "Fitting on Testing Data and degreee: 1:  1.140368874538823\n",
      "Fitting on Testing Data and degreee: 2:  1.180767966877412\n",
      "Fitting on Testing Data and degreee: 10:  1.7503153161323404\n",
      "Fitting on Testing Data and degreee: 20:  2.3078012120038514\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# Ridge penalty values\n",
    "alphas = [0, 0.1, 0.5, 1]\n",
    "\n",
    "# Polynomial degree values\n",
    "gammas = [1, 2, 10, 20]\n",
    "mspe_array = np.zeros((len(alphas), len(gammas)))\n",
    "\n",
    "# Iterate over alphas and gammas, and fill the MSPE values\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "\n",
    "        model = KernelRidge(alpha=alpha, kernel=\"laplacian\", gamma = gamma, coef0=1)\n",
    "\n",
    "        crossValErrors = kFoldMspeCalc(kf, model, X, y)\n",
    "        # Print mean MSPE\n",
    "        print(\"Polynomial Kernel MSPE with lambda: \"+str(alpha)+\" and degree: \"+str(gamma)+\" is:\" ,np.mean(crossValErrors))\n",
    "    print()\n",
    "\n",
    "print(\"lambda =1 yields the best results. Running predictions with lambda =0.5\")\n",
    "for gamma in gammas:\n",
    "    model = KernelRidge(alpha=0.5, kernel=\"laplacian\", gamma = gamma)\n",
    "    print(\"Fitting on Testing Data and degreee: \"+str(gamma)+\": \", testingMSPE(model, X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF does not significantly outperform the polynomial kernel at degree 2, it would appear polynomial kernels do appear to get significant performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. (10 pts) Investigate the utility of random Fourier feature expansion in improving the prediction\n",
    "of motor UPDRS scores for the polynomial feature maps of order 2 and 5, respectively. Vary the\n",
    "random feature dimension as 6, 12, and 24. How do the random features compare to previous\n",
    "methods?</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF Kernel with 6 random features with polynomial degree 2 is: 1.0330527877498334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF Kernel with 12 random features with polynomial degree 2 is: 0.981770286108666\n",
      "RBF Kernel with 24 random features with polynomial degree 2 is: 0.970854962484076\n",
      "RBF Kernel with 6 random features with polynomial degree 5 is: 0.9844314154178073\n",
      "RBF Kernel with 12 random features with polynomial degree 5 is: 0.9890882083542618\n",
      "RBF Kernel with 24 random features with polynomial degree 5 is: 0.96286860633002\n",
      "\n",
      "Fitting on Testing Data:  1.1216076055097337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create a list of random feature dimensions\n",
    "random_feature_dims = [6, 12, 24]\n",
    "\n",
    "\n",
    "# Loop over polynomial degrees\n",
    "for degree in [2, 5]:\n",
    "        \n",
    "        # Create a PolynomialFeatures object\n",
    "        poly = PolynomialFeatures(degree=degree)\n",
    "        \n",
    "        # Loop over random feature dimensions\n",
    "        for n_components in random_feature_dims:       \n",
    "            # Create a pipeline with RBFSampler and Ridge regression\n",
    "            rbf_sampler = RBFSampler(n_components=n_components, gamma=1.0)\n",
    "            ridge = Ridge()\n",
    "            model = Pipeline([(\"rbf_sampler\", rbf_sampler),\n",
    "                            (\"poly_features\", poly),\n",
    "                            (\"ridge\", ridge)])\n",
    "            \n",
    "            # Perform 10-fold cross-validation and compute the MSPE\n",
    "            crossValErrors = kFoldMspeCalc(kf, model, X, y)\n",
    "            mspe = np.mean(crossValErrors)\n",
    "            print(\"RBF Kernel with \"+str(n_components)+\" random features with polynomial degree \"+str(degree)+\" is:\" ,np.mean(crossValErrors))\n",
    "print()\n",
    "\n",
    "degree = 5\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "rbf_sampler = RBFSampler(n_components=n_components, gamma=1.0)\n",
    "model = Pipeline([(\"rbf_sampler\", rbf_sampler),\n",
    "                            (\"poly_features\", poly),\n",
    "                            (\"ridge\", ridge)])\n",
    "\n",
    "print(\"Fitting on Testing Data: \", testingMSPE(model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Fourier features do give a pretty good performance on the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6. (10 pts) Fit a two-layered (shallow) neural network for predicting motor UPDRS scores. Is its performance better than that of the previous methods? Justify the choice of tuning parameters such as number of hidden units, pre-activation function, learning rate, and epochs.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Layer NN with 16, activation identity learning_rate:0.001 MSPE is: 1.378090734400517\n",
      "2 Layer NN with 16, activation identity learning_rate:0.01 MSPE is: 0.9938879032116505\n",
      "2 Layer NN with 16, activation identity learning_rate:0.1 MSPE is: 1.1823460416531895\n",
      "2 Layer NN with 16, activation relu learning_rate:0.001 MSPE is: 1.262759670845105\n",
      "2 Layer NN with 16, activation relu learning_rate:0.01 MSPE is: 1.0492827125372273\n",
      "2 Layer NN with 16, activation relu learning_rate:0.1 MSPE is: 1.3495954958700165\n",
      "2 Layer NN with 16, activation tanh learning_rate:0.001 MSPE is: 1.405237626351632\n",
      "2 Layer NN with 16, activation tanh learning_rate:0.01 MSPE is: 1.0288073409279996\n",
      "2 Layer NN with 16, activation tanh learning_rate:0.1 MSPE is: 1.166715972571512\n",
      "2 Layer NN with 16, activation logistic learning_rate:0.001 MSPE is: 1.8154281419371994\n",
      "2 Layer NN with 16, activation logistic learning_rate:0.01 MSPE is: 1.013769475687628\n",
      "2 Layer NN with 16, activation logistic learning_rate:0.1 MSPE is: 1.1884762776702804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "for size in [16]:\n",
    "    for activation in [\"identity\", \"relu\", \"tanh\", \"logistic\"]:\n",
    "        for learning_rate in [0.001, 0.01, 0.1]:\n",
    "            # Create the neural network model\n",
    "            model = MLPRegressor(\n",
    "                hidden_layer_sizes=(size,),  # Number of hidden units\n",
    "                activation=activation,  # Activation function for hidden layers\n",
    "                solver='adam',  # Optimization algorithm\n",
    "                alpha=0.0001,  # L2 regularization parameter\n",
    "                batch_size='auto',  # Batch size for optimization\n",
    "                learning_rate_init=learning_rate,  # Initial learning rate\n",
    "                max_iter=500,  # Maximum number of iterations\n",
    "                shuffle=True,  # Shuffle the data before each iteration\n",
    "                random_state=42,  # Random state for reproducibility\n",
    "                n_iter_no_change=10  # Maximum number of iterations with no improvement\n",
    "            )\n",
    "\n",
    "            crossValErrors = kFoldMspeCalc(kf, model, X, y)\n",
    "\n",
    "            # Print mean MSPE\n",
    "            print(\"2 Layer NN with \"+str(size)+\", activation \"+activation+\" learning_rate:\"+str(learning_rate)+\" MSPE is:\" ,np.mean(crossValErrors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function identity(linear) and learning rate: 0.01 is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Layer NN with 16, activation identity learning_rate:0.01 MSPE is: 0.9938879032116505\n",
      "2 Layer NN with 64, activation identity learning_rate:0.01 MSPE is: 1.0540220970488563\n",
      "2 Layer NN with 256, activation identity learning_rate:0.01 MSPE is: 1.0257832423290403\n",
      "2 Layer NN with 1024, activation identity learning_rate:0.01 MSPE is: 1.0427417934360284\n",
      "2 Layer NN with 4096, activation identity learning_rate:0.01 MSPE is: 1.4157505246558837\n",
      "Fitting on Testing Data with identity @ 0.01 learning rate and 64 hidden units:  1.160988882407963\n"
     ]
    }
   ],
   "source": [
    " \n",
    "for size in [16, 64, 256, 1024, 4096]:\n",
    "    for activation in [\"identity\"]:\n",
    "        for learning_rate in [0.01]:\n",
    "            # Create the neural network model\n",
    "            model = MLPRegressor(\n",
    "                hidden_layer_sizes=(size,),  # Number of hidden units\n",
    "                activation=activation,  # Activation function for hidden layers\n",
    "                solver='adam',  # Optimization algorithm\n",
    "                alpha=0.0001,  # L2 regularization parameter\n",
    "                batch_size='auto',  # Batch size for optimization\n",
    "                learning_rate_init=learning_rate,  # Initial learning rate\n",
    "                max_iter=500,  # Maximum number of iterations\n",
    "                shuffle=True,  # Shuffle the data before each iteration\n",
    "                random_state=42,  # Random state for reproducibility\n",
    "                n_iter_no_change=10  # Maximum number of iterations with no improvement\n",
    "            )\n",
    "\n",
    "            crossValErrors = kFoldMspeCalc(kf, model, X, y)\n",
    "\n",
    "            # Print mean MSPE\n",
    "            print(\"2 Layer NN with \"+str(size)+\", activation \"+activation+\" learning_rate:\"+str(learning_rate)+\" MSPE is:\" ,np.mean(crossValErrors))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = MLPRegressor(\n",
    "                hidden_layer_sizes=(16,),  # Number of hidden units\n",
    "                activation=\"identity\",  # Activation function for hidden layers\n",
    "                solver='adam',  # Optimization algorithm\n",
    "                alpha=0.0001,  # L2 regularization parameter\n",
    "                batch_size='auto',  # Batch size for optimization\n",
    "                learning_rate_init=0.01,  # Initial learning rate\n",
    "                max_iter=500,  # Maximum number of iterations\n",
    "                shuffle=True,  # Shuffle the data before each iteration\n",
    "                random_state=42,  # Random state for reproducibility\n",
    "                n_iter_no_change=10  # Maximum number of iterations with no improvement\n",
    "            )\n",
    "print(\"Fitting on Testing Data with identity @ 0.01 learning rate and 16 hidden units: \", testingMSPE(model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.01 initial learning rate with relu activation with 16 hidden units is best in terms of training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of hidden units: The number of hidden units (64 in this example) controls the complexity of the neural network model. More hidden units can capture more complex patterns in the data.\n",
    "\n",
    "Activation function: The Identity activation function is a commonly used in regression tasks.\n",
    "\n",
    "Learning rate: The learning rate (0.001 in this example) controls the step size of the optimization algorithm. A smaller learning rate may converge more slowly but may be more stable, while a larger learning rate may converge faster but may be less stable. You can try different values or use a learning rate scheduler.\n",
    "\n",
    "Epochs (max_iter): The number of epochs (500 in this example) determines how many times the entire dataset is used for training. More epochs may lead to better performance but also increase the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>7. (10 pts) Compare the predictive performance when a deep neural network replaces the shallow\n",
    "neural networks.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Layer NN MSPE is: 1.0525286133780953\n",
      "Fitting on 5 layer netwrok with relu @ 0.01 learning rate :  1.125447688230306\n"
     ]
    }
   ],
   "source": [
    "# Create the neural network model\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(8,8,8,8,8),  # 8 hidden units in 5 layers\n",
    "    activation='relu',  # Activation function for hidden layers\n",
    "    solver='adam',  # Optimization algorithm\n",
    "    alpha=0.0001,  # L2 regularization parameter\n",
    "    batch_size='auto',  # Batch size for optimization\n",
    "    learning_rate_init=0.01,  # Initial learning rate\n",
    "    max_iter=500,  # Maximum number of iterations\n",
    "    shuffle=True,  # Shuffle the data before each iteration\n",
    "    random_state=42,  # Random state for reproducibility\n",
    "    n_iter_no_change=10  # Maximum number of iterations with no improvement\n",
    ")\n",
    "\n",
    "crossValErrors = kFoldMspeCalc(kf, model, X, y)\n",
    "\n",
    "# Print mean MSPE\n",
    "print(\"5 Layer NN MSPE is:\" ,np.mean(crossValErrors))\n",
    "\n",
    "print(\"Fitting on 5 layer netwrok with relu @ 0.01 learning rate : \", testingMSPE(model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Layer NN MSPE is: 1.0463081168309047\n",
      "Fitting on 5 layer netwrok with relu @ 0.01 learning rate :  1.1208587261879228\n"
     ]
    }
   ],
   "source": [
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(16,16,16,16,16),  # 16 hidden units in 5 layers\n",
    "    activation='relu',  # Activation function for hidden layers\n",
    "    solver='adam',  # Optimization algorithm\n",
    "    alpha=0.0001,  # L2 regularization parameter\n",
    "    batch_size='auto',  # Batch size for optimization\n",
    "    learning_rate_init=0.01,  # Initial learning rate\n",
    "    max_iter=500,  # Maximum number of iterations\n",
    "    shuffle=True,  # Shuffle the data before each iteration\n",
    "    random_state=42,  # Random state for reproducibility\n",
    "    n_iter_no_change=10  # Maximum number of iterations with no improvement\n",
    ")\n",
    "\n",
    "crossValErrors = kFoldMspeCalc(kf, model, X, y)\n",
    "\n",
    "# Print mean MSPE\n",
    "print(\"5 Layer NN MSPE is:\" ,np.mean(crossValErrors))\n",
    "\n",
    "print(\"Fitting on 5 layer netwrok with relu @ 0.01 learning rate : \", testingMSPE(model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Layer NN MSPE is: 1.3712590882154783\n",
      "Fitting on 5 layer netwrok with relu @ 0.01 learning rate :  1.2790406570357609\n"
     ]
    }
   ],
   "source": [
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(16,16,16,16,16, 16, 16,16,16,16),  # 16 hidden units in 10 layers\n",
    "    activation='relu',  # Activation function for hidden layers\n",
    "    solver='adam',  # Optimization algorithm\n",
    "    alpha=0.0001,  # L2 regularization parameter\n",
    "    batch_size='auto',  # Batch size for optimization\n",
    "    learning_rate_init=0.01,  # Initial learning rate\n",
    "    max_iter=500,  # Maximum number of iterations\n",
    "    shuffle=True,  # Shuffle the data before each iteration\n",
    "    random_state=42,  # Random state for reproducibility\n",
    "    n_iter_no_change=10  # Maximum number of iterations with no improvement\n",
    ")\n",
    "\n",
    "crossValErrors = kFoldMspeCalc(kf, model, X, y)\n",
    "\n",
    "# Print mean MSPE\n",
    "print(\"5 Layer NN MSPE is:\" ,np.mean(crossValErrors))\n",
    "\n",
    "print(\"Fitting on 5 layer netwrok with relu @ 0.01 learning rate : \", testingMSPE(model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 layers with 16 hidden units seems to work best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>8. (10 pts) Assess and compare the MSPEs of all the regression models from (i) to (vii). Comment\n",
    "on this flexibility and their dependence on the tuning parameters. For example, does the ranks\n",
    "of MSPEs match the rankings of the flexibility?</h3>\n",
    "\n",
    "Flexibility does seem to effect testing MSPE generally as we went from having a 1.18 testing MSPE on linear values to an MSPE of approx 1.12 with neural networks. Some regressors were highly dependent on tuning params like RBF but overall the performance was not drastically different betweeen models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>9. (10 pts) Perform a sensitivity analysis on the tuning parameters, such as degree of the polynomial, regularization parameter, random feature dimension, learning rate, number of layers, and number of epochs. How do changes in these parameters affect the model’s performance? Which model balances sensitivity to the choice of tuning parameter and predictive accuracy?</h3>\n",
    "\n",
    "Kernels with penalties like poynomial ridge and PCA were dependent on their tuning. Increasing model complexity i.e. model degree does not always help as bais variance tradeoff seems to be at play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
