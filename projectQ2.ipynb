{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2\n",
    "This question involves creating a statistical model that can differentiate between authentic and\n",
    "counterfeit banknotes. We utilize the Banknote Authentication dataset from the UCI Machine\n",
    "Learning Repository, provided by Volker Lohweg at the University of Applied Sciences, OstwestfalenLippe. Accessible at (https://archive.ics.uci.edu/ml/datasets/banknote+authentication),\n",
    "the dataset comprises 1372 samples derived from images of real and fake banknotes. These samples\n",
    "are contained in the banknote.csv file. Each sample is described by five attributes: one binary\n",
    "response variable and four predictors. The response variable is 1 for genuine notes and 0 for forgeries. The predictors, extracted using wavelet transform, include measurements of image variance,\n",
    "skewness, kurtosis, and entropy. Table 2 provides a detailed description of the variables in the\n",
    "banknote data.\n",
    "Variable Description\n",
    "class Response taking two values: 0 for a forged banknote and 1 for a genuine banknote\n",
    "variance Variance of wavelet transformed banknote image\n",
    "skewness Skewness of wavelet transformed banknote image\n",
    "curtosis Curtosis of wavelet transformed banknote image\n",
    "entropy Entropy of the banknote image\n",
    "Table 2: Description of the response and predictors in banknote.csv file. The response is in the\n",
    "first row and the remaining rows describe the four predictors, which are continuous and can take\n",
    "any real values.\n",
    "Answer the following questions based on classification models. Evaluate their predictive performance using accuracy, TPR, and FPR estimates based on 10-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (10 pts) How does logistic regression and the perceptron perform in classifying banknotes as\n",
    "genuine or counterfeit based on the four given image-derived features? Summarize predictive\n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy  class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('banknote.csv')\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = data.iloc[:, 0:4].values  # Features\n",
    "\n",
    "y = data.iloc[:,-1].values   # Target variable\n",
    "\n",
    "# Function to evaluate model by k-fold cross validation\n",
    "# Inputs: k-fold division, the model, X matrix and y matrix \n",
    "# Output: Accuracy, TPR, FPR metrics\n",
    "# Process: Divide the data into k folds. For each fold, train the data on other k-1 folds\n",
    "# and calculate mspe on the current fold.    Store the values in an array\n",
    "def kFoldModelEvaluator(kFold, model, X, y):\n",
    "    # Define a matrix for storing metrics\n",
    "\n",
    "    accuracyScores = []\n",
    "    tprScores = []\n",
    "    fprScores = []\n",
    "    \n",
    "    # Perform 10-fold cross-validation\n",
    "    for train_index, test_index in kFold.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracyScores.append(model.score(X_test, y_test))\n",
    "\n",
    "        # Predict on testing data\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate TPR and FPR\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        tprScores.append( tp / (tp + fn))\n",
    "        fprScores.append( fp / (fp + tn))\n",
    "    \n",
    "    return {\"accuracyScores\": accuracyScores, \"tprScores\" : tprScores, \"fprScores\" : fprScores}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (10 pts) How does logistic regression and the perceptron perform in classifying banknotes as\n",
    "genuine or counterfeit based on the four given image-derived features? Summarize predictive\n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9898021792023695\n",
      "TPR:  0.9888331762323699\n",
      "FPR:  0.009328434706096041\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "\n",
    "\n",
    "model = LogisticRegression(penalty=None)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Accuracy: \", np.mean(kFoldModelEvaluator(kf, model, X, y).get(\"accuracyScores\")))\n",
    "print(\"TPR: \", np.mean(kFoldModelEvaluator(kf, model, X, y).get(\"tprScores\")))\n",
    "print(\"FPR: \", np.mean(kFoldModelEvaluator(kf, model, X, y).get(\"fprScores\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9839627631439753\n",
      "TPR:  0.9785143012113953\n",
      "FPR:  0.011619557989003363\n"
     ]
    }
   ],
   "source": [
    "model = Perceptron(penalty=None)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Accuracy: \", np.mean(kFoldModelEvaluator(kf, model, X, y).get(\"accuracyScores\")))\n",
    "print(\"TPR: \", np.mean(kFoldModelEvaluator(kf, model, X, y).get(\"tprScores\")))\n",
    "print(\"FPR: \", np.mean(kFoldModelEvaluator(kf, model, X, y).get(\"fprScores\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression seems to have marginally better accuracy on average of 98.9% vs. 98.3 of the perceptron. It also has a better TPR and FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (10 pts) Assess the predictive performance improvement of logistic regression and perceptron with polynomial features of degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "TPR:  1.0\n",
      "FPR:  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "X_poly = poly.fit_transform(X)\n",
    "model = LogisticRegression(penalty=None)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Accuracy: \", np.mean(kFoldModelEvaluator(kf, model, X_poly, y).get(\"accuracyScores\")))\n",
    "print(\"TPR: \", np.mean(kFoldModelEvaluator(kf, model, X_poly, y).get(\"tprScores\")))\n",
    "print(\"FPR: \", np.mean(kFoldModelEvaluator(kf, model, X_poly, y).get(\"fprScores\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9985401459854014\n",
      "TPR:  0.9966517857142858\n",
      "FPR:  0.0\n"
     ]
    }
   ],
   "source": [
    "model = Perceptron(penalty=None)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Accuracy: \", np.mean(kFoldModelEvaluator(kf, model, X_poly, y).get(\"accuracyScores\")))\n",
    "print(\"TPR: \", np.mean(kFoldModelEvaluator(kf, model, X_poly, y).get(\"tprScores\")))\n",
    "print(\"FPR: \", np.mean(kFoldModelEvaluator(kf, model, X_poly, y).get(\"fprScores\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Quadratic features based Logistic Regression seems to be able to achieve perfect separation on the data. Percpetron also nearly manages to achieve the same with a few false negative errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (10 pts) Compare the predictive performance of the previous models with SVMs with polynomial\n",
    "kernels of degrees 1, 2, 10, and 20. Does SVM perform better than the previous approaches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for degree1:  0.9846926901512747\n",
      "TPR for degree1:  0.9984848484848484\n",
      "FPR for degree1:  0.02596872678836103\n",
      "Accuracy for degree2:  0.9730297260129059\n",
      "TPR for degree2:  1.0\n",
      "FPR for degree2:  0.04827484353572536\n",
      "Accuracy for degree10:  0.7653549137839839\n",
      "TPR for degree10:  0.557826547979445\n",
      "FPR for degree10:  0.05838144965486223\n",
      "Accuracy for degree20:  0.7150481328678727\n",
      "TPR for degree20:  0.3666633172124153\n",
      "FPR for degree20:  0.004938647997591086\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "for degree in [1,2,10,20]:\n",
    "    model = svm.SVC(kernel=\"poly\", degree = degree)\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    print(\"Accuracy for degree\"+str(degree)+\": \", np.mean(kFoldModelEvaluator(kf, model, X, y).get(\"accuracyScores\")))\n",
    "    print(\"TPR for degree\"+str(degree)+\": \", np.mean(kFoldModelEvaluator(kf, model, X, y).get(\"tprScores\")))\n",
    "    print(\"FPR for degree\"+str(degree)+\": \", np.mean(kFoldModelEvaluator(kf, model, X, y).get(\"fprScores\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier seems to not perform well for 10 and 20 degree models at all, with a .56 and .37 TPRs respectively. 2nd degree kernel does seem to reach separation and a perfect TPR although its false positive rate still lags behind Logistic Regression and Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (10 pts) Implement a kernel approximation using random Fourier features for banknote classification using logistic regression with degree 3 polynomial features. Vary the random feature\n",
    "dimension as 6, 12, and 24. How does this approach compare with the SVM in terms of predictive\n",
    "performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for components6:  0.5430180894953984\n",
      "TPR for components6:  0.07378987445320946\n",
      "FPR for components6:  0.07823590198130417\n",
      "Accuracy for components12:  0.5219083888712578\n",
      "TPR for components12:  0.16578949331480483\n",
      "FPR for components12:  0.18728302618683104\n",
      "Accuracy for components24:  0.518978102189781\n",
      "TPR for components24:  0.2476027461603516\n",
      "FPR for components24:  0.2616714700295687\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create a list of random feature dimensions\n",
    "random_feature_dims = [6, 12, 24]\n",
    "\n",
    "# Create a PolynomialFeatures object\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "        \n",
    "# Loop over random feature dimensions\n",
    "for n_components in random_feature_dims:       \n",
    "    # Create a pipeline with RBFSampler for fourier features\n",
    "    rbf_sampler = RBFSampler(n_components=n_components, gamma=1.0)\n",
    "    logreg = LogisticRegression(penalty=None, max_iter=5000)\n",
    "    model = Pipeline([(\"rbf_sampler\", rbf_sampler),\n",
    "                    \n",
    "                            (\"logreg\", logreg)])\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    res = kFoldModelEvaluator(kf, model, X_poly, y)\n",
    "\n",
    "    print(\"Accuracy for components\"+str(n_components)+\": \", np.mean(res.get(\"accuracyScores\")))\n",
    "    print(\"TPR for components\"+str(n_components)+\": \", np.mean(res.get(\"tprScores\")))\n",
    "    print(\"FPR for components\"+str(n_components)+\": \", np.mean(res.get(\"fprScores\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random feature kernel does not work as well, with the best preformance coming from 24 components. This indicates more components are required to increase accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (10 pts) Fit a two-layered (shallow) neural network. Is its performance better than that of the previous methods? Justify the choice of tuning parameters such as number of hidden units, pre-activation function, learning rate, and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for activation:relu, learning_rate:0.001:  1.0\n",
      "TPR for activation:relu, learning_rate:0.001:  1.0\n",
      "FPR for activation:relu, learning_rate:0.001:  0.0\n",
      "Accuracy for activation:relu, learning_rate:0.01:  1.0\n",
      "TPR for activation:relu, learning_rate:0.01:  1.0\n",
      "FPR for activation:relu, learning_rate:0.01:  0.0\n",
      "Accuracy for activation:relu, learning_rate:0.05:  1.0\n",
      "TPR for activation:relu, learning_rate:0.05:  1.0\n",
      "FPR for activation:relu, learning_rate:0.05:  0.0\n",
      "Accuracy for activation:relu, learning_rate:0.1:  1.0\n",
      "TPR for activation:relu, learning_rate:0.1:  1.0\n",
      "FPR for activation:relu, learning_rate:0.1:  0.0\n",
      "Accuracy for activation:relu, learning_rate:1:  1.0\n",
      "TPR for activation:relu, learning_rate:1:  1.0\n",
      "FPR for activation:relu, learning_rate:1:  0.0\n",
      "Accuracy for activation:logistic, learning_rate:0.001:  0.991262033216968\n",
      "TPR for activation:logistic, learning_rate:0.001:  0.9969696969696968\n",
      "FPR for activation:logistic, learning_rate:0.001:  0.013182865621029239\n",
      "Accuracy for activation:logistic, learning_rate:0.01:  1.0\n",
      "TPR for activation:logistic, learning_rate:0.01:  1.0\n",
      "FPR for activation:logistic, learning_rate:0.01:  0.0\n",
      "Accuracy for activation:logistic, learning_rate:0.05:  1.0\n",
      "TPR for activation:logistic, learning_rate:0.05:  1.0\n",
      "FPR for activation:logistic, learning_rate:0.05:  0.0\n",
      "Accuracy for activation:logistic, learning_rate:0.1:  1.0\n",
      "TPR for activation:logistic, learning_rate:0.1:  1.0\n",
      "FPR for activation:logistic, learning_rate:0.1:  0.0\n",
      "Accuracy for activation:logistic, learning_rate:1:  0.9992700729927007\n",
      "TPR for activation:logistic, learning_rate:1:  0.9983870967741936\n",
      "FPR for activation:logistic, learning_rate:1:  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "for activation in [\"relu\", \"logistic\"]:\n",
    "    for learning_rate in [0.001, 0.01, 0.05, 0.1, 1]:\n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes= (20,),\n",
    "            activation=activation,\n",
    "            learning_rate_init=learning_rate,\n",
    "            max_iter= 5000 \n",
    "        )\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "        res = kFoldModelEvaluator(kf, model, X, y)\n",
    "\n",
    "        print(\"Accuracy for activation:\"+activation+\", learning_rate:\"+str(learning_rate)+\": \", np.mean(res.get(\"accuracyScores\")))\n",
    "        print(\"TPR for activation:\"+activation+\", learning_rate:\"+str(learning_rate)+\": \", np.mean(res.get(\"tprScores\")))\n",
    "        print(\"FPR for activation:\"+activation+\", learning_rate:\"+str(learning_rate)+\": \", np.mean(res.get(\"fprScores\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing ReLU with 0.01 initial learning rate as this value has achieved convergance.\n",
    "\n",
    "Now testing on hidden unit size and max iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for size:2, max_iter:10:  0.7318152967311965\n",
      "TPR for size:2, max_iter:10:  0.7189076160189166\n",
      "FPR for size:2, max_iter:10:  0.26819391453879565\n",
      "Accuracy for size:2, max_iter:100:  0.9905321062096689\n",
      "TPR for size:2, max_iter:100:  0.9903632086999024\n",
      "FPR for size:2, max_iter:100:  0.009463729818560102\n",
      "Accuracy for size:2, max_iter:500:  0.9956257272823443\n",
      "TPR for size:2, max_iter:500:  0.996875\n",
      "FPR for size:2, max_iter:500:  0.005288427326098559\n",
      "Accuracy for size:2, max_iter:1000:  0.9963556542896435\n",
      "TPR for size:2, max_iter:1000:  0.9952116935483872\n",
      "FPR for size:2, max_iter:1000:  0.00251821349382325\n",
      "Accuracy for size:4, max_iter:10:  0.9241986670898129\n",
      "TPR for size:4, max_iter:10:  0.9430903318057402\n",
      "FPR for size:4, max_iter:10:  0.08994532268950356\n",
      "Accuracy for size:4, max_iter:100:  1.0\n",
      "TPR for size:4, max_iter:100:  1.0\n",
      "FPR for size:4, max_iter:100:  0.0\n",
      "Accuracy for size:4, max_iter:500:  1.0\n",
      "TPR for size:4, max_iter:500:  1.0\n",
      "FPR for size:4, max_iter:500:  0.0\n",
      "Accuracy for size:4, max_iter:1000:  1.0\n",
      "TPR for size:4, max_iter:1000:  1.0\n",
      "FPR for size:4, max_iter:1000:  0.0\n",
      "Accuracy for size:10, max_iter:10:  0.986157833492013\n",
      "TPR for size:10, max_iter:10:  0.9852878935349041\n",
      "FPR for size:10, max_iter:10:  0.013511736538939842\n",
      "Accuracy for size:10, max_iter:100:  1.0\n",
      "TPR for size:10, max_iter:100:  1.0\n",
      "FPR for size:10, max_iter:100:  0.0\n",
      "Accuracy for size:10, max_iter:500:  1.0\n",
      "TPR for size:10, max_iter:500:  1.0\n",
      "FPR for size:10, max_iter:500:  0.0\n",
      "Accuracy for size:10, max_iter:1000:  1.0\n",
      "TPR for size:10, max_iter:1000:  1.0\n",
      "FPR for size:10, max_iter:1000:  0.0\n",
      "Accuracy for size:20, max_iter:10:  1.0\n",
      "TPR for size:20, max_iter:10:  1.0\n",
      "FPR for size:20, max_iter:10:  0.0\n",
      "Accuracy for size:20, max_iter:100:  1.0\n",
      "TPR for size:20, max_iter:100:  1.0\n",
      "FPR for size:20, max_iter:100:  0.0\n",
      "Accuracy for size:20, max_iter:500:  1.0\n",
      "TPR for size:20, max_iter:500:  1.0\n",
      "FPR for size:20, max_iter:500:  0.0\n",
      "Accuracy for size:20, max_iter:1000:  1.0\n",
      "TPR for size:20, max_iter:1000:  1.0\n",
      "FPR for size:20, max_iter:1000:  0.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for size in [2, 4, 10, 20]:\n",
    "    for max_iter in [10, 100, 500, 1000]:\n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes= (size,),\n",
    "            activation=\"relu\",\n",
    "            learning_rate_init=0.01,\n",
    "            max_iter= max_iter \n",
    "        )\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "        res = kFoldModelEvaluator(kf, model, X, y)\n",
    "\n",
    "        print(\"Accuracy for size:\"+str(size)+\", max_iter:\"+str(max_iter)+\": \", np.mean(res.get(\"accuracyScores\")))\n",
    "        print(\"TPR for size:\"+str(size)+\", max_iter:\"+str(max_iter)+\": \", np.mean(res.get(\"tprScores\")))\n",
    "        print(\"FPR for size:\"+str(size)+\", max_iter:\"+str(max_iter)+\": \", np.mean(res.get(\"fprScores\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can converge with just 4 hidden units, given it has a sufficient size of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. (10 pts) Compare the predictive performance when a deep neural network replaces the shallow\n",
    "neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for size:(4,4,4),:  1.0\n",
      "TPR for size:(4,4,4), :  1.0\n",
      "FPR for size:(4,4,4), :  0.0\n"
     ]
    }
   ],
   "source": [
    "max_iter = 500\n",
    "\n",
    "model = MLPClassifier(\n",
    "            hidden_layer_sizes= (4,4,4),\n",
    "            activation=\"relu\",\n",
    "            learning_rate_init=0.01,\n",
    "            max_iter= 500 \n",
    ")\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "res = kFoldModelEvaluator(kf, model, X, y)\n",
    "\n",
    "print(\"Accuracy for size:(4,4,4),: \", np.mean(res.get(\"accuracyScores\")))\n",
    "print(\"TPR for size:(4,4,4), : \", np.mean(res.get(\"tprScores\")))\n",
    "print(\"FPR for size:(4,4,4), : \", np.mean(res.get(\"fprScores\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A four layered network can replace the given network if it has upto 500 iterations available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. (10 pts) Perform a sensitivity analysis on the tuning parameters, such as degree of the polynomial, regularization parameter, random feature dimension, learning rate, number of layers, and\n",
    "number of epochs. How do changes in these parameters affect the model’s performance? Which\n",
    "model balances sensitivity to the choice of tuning parameter and predictive accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of components in a random feature kernel has the highest sensitivity in terms of change in output. The iterations for this model seem to be sufficient for convergance at 100 to 500 range, while the initial Learning rate is best between 0.01 and 0.05 to ensure best convergance. Quadratice features seem sufficient for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
